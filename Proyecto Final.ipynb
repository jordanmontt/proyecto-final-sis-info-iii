{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28046</th>\n",
       "      <td>Rented a batch of films from Blockbuster last ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38416</th>\n",
       "      <td>Dick Clement and Ian La Frenais have a solid h...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36230</th>\n",
       "      <td>Ugly shot, poorly scripted and amateurishly pa...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29802</th>\n",
       "      <td>Margret Laurence probably didn't intend on hav...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19238</th>\n",
       "      <td>***SPOILERS*** Well made and interesting film ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40691</th>\n",
       "      <td>I just saw this cartoon for the first time and...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22733</th>\n",
       "      <td>Last time I checked in here I think there was ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>carrot top in a full length movie, enough said...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34524</th>\n",
       "      <td>I'll give credit where credit is due, and say ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>Okay, so it was never going to change the worl...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Label\n",
       "28046  Rented a batch of films from Blockbuster last ...   neg\n",
       "38416  Dick Clement and Ian La Frenais have a solid h...   pos\n",
       "36230  Ugly shot, poorly scripted and amateurishly pa...   neg\n",
       "29802  Margret Laurence probably didn't intend on hav...   neg\n",
       "19238  ***SPOILERS*** Well made and interesting film ...   pos\n",
       "...                                                  ...   ...\n",
       "40691  I just saw this cartoon for the first time and...   pos\n",
       "22733  Last time I checked in here I think there was ...   pos\n",
       "8746   carrot top in a full length movie, enough said...   neg\n",
       "34524  I'll give credit where credit is due, and say ...   neg\n",
       "10499  Okay, so it was never going to change the worl...   neg\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "ruta_archivo = os.path.join(\"imdb_dataset.csv\")\n",
    "df_criticas = pd.read_csv(ruta_archivo, encoding='iso-8859-2').sample(1000, replace=False)\n",
    "#df_criticas = pd.read_csv(ruta_archivo, encoding='iso-8859-2')\n",
    "df_criticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras de parada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "palabras_de_parada_ingles = set(nltk.corpus.stopwords.words('english') + list(string.punctuation) + ['...', '..'])\n",
    "palabras_de_parada_ingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28046</th>\n",
       "      <td>rented batch films blockbuster last night firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38416</th>\n",
       "      <td>dick clement ian la frenais solid hit rate far...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36230</th>\n",
       "      <td>ugly shot poorly scripted amateurishly paced s...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29802</th>\n",
       "      <td>margret laurence probably intend novels adopte...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19238</th>\n",
       "      <td>spoilers well made interesting film alienated ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40691</th>\n",
       "      <td>saw cartoon first time recognized caricatures ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22733</th>\n",
       "      <td>last time checked think one comment glad peopl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>carrot top full length movie enough said reaso...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34524</th>\n",
       "      <td>give credit credit due say linda fiorentino gi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>okay never going change world bombed box offic...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Label\n",
       "28046  rented batch films blockbuster last night firs...   neg\n",
       "38416  dick clement ian la frenais solid hit rate far...   pos\n",
       "36230  ugly shot poorly scripted amateurishly paced s...   neg\n",
       "29802  margret laurence probably intend novels adopte...   neg\n",
       "19238  spoilers well made interesting film alienated ...   pos\n",
       "...                                                  ...   ...\n",
       "40691  saw cartoon first time recognized caricatures ...   pos\n",
       "22733  last time checked think one comment glad peopl...   pos\n",
       "8746   carrot top full length movie enough said reaso...   neg\n",
       "34524  give credit credit due say linda fiorentino gi...   neg\n",
       "10499  okay never going change world bombed box offic...   neg\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LimpiadorTexto es una clase que nosotros creamos.\n",
    "# Utilizamos la funcion normalizar aquí, en el notebook. y también en la API.\n",
    "# Por tanto, para no tener código repetido, abstraímos la lógica a una clase externa.\n",
    "from LimpiadorTexto import LimpiadorTexto\n",
    "limpiador = LimpiadorTexto(palabras_de_parada_ingles)\n",
    "df_criticas = df_criticas.apply(limpiador.normalizar_fila, axis=1)\n",
    "df_criticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener el vocabulario del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_vocabulario_problema(df_criticas_normalizadas):\n",
    "    cuerpo_texto_normalizado = ' '.join(df_criticas_normalizadas['Review'].tolist())\n",
    "    vocabulario_problema = cuerpo_texto_normalizado.split()\n",
    "    vocabulario_ordenado = sorted(set(vocabulario_problema))\n",
    "    return vocabulario_ordenado\n",
    "\n",
    "def obtener_vocabulario_problema_y_posicion(vocabulario_del_problema_ordenado):\n",
    "    vocabulario_y_posicion = {}\n",
    "    for i, token in enumerate(vocabulario_del_problema_ordenado):\n",
    "        vocabulario_y_posicion[token] = i\n",
    "    return vocabulario_y_posicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21079"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario_problema = obtener_vocabulario_problema_y_posicion(obtener_vocabulario_problema(df_criticas))\n",
    "len(vocabulario_problema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def obtener_one_hot_vector(critica, vocabulario_problema_y_posicion):\n",
    "    one_hot_vector = np.zeros(len(vocabulario_problema_y_posicion), dtype=int)\n",
    "    for token in critica.split():\n",
    "        one_hot_vector[vocabulario_problema_y_posicion[token]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots = []\n",
    "indices = []\n",
    "for indice, fila in df_criticas.iterrows():\n",
    "    one_hot = obtener_one_hot_vector(fila['Review'], vocabulario_problema)\n",
    "    indices.append(indice)\n",
    "    one_hots.append(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crear y entrenar el modelo predictivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# se cambia el nombre de la variable 'one_hots' para mas claridad\n",
    "X = one_hots\n",
    "Y = df_criticas['Label'].ravel()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train =  np.array(X_train)\n",
    "X_test =  np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberar la memoria RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "lst = [df_criticas]\n",
    "del df_criticas, one_hots, X, Y\n",
    "del lst\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar diferentes modelos con diferentes parámetros para encontrar el que produzca mejores resultados\n",
    "\n",
    "Se entrenarán diferentes modelos con diferentes parámetros. Se guardará las métricas de cada modelo en un archivo para que luego puedan ser comparadas, y así determinar el mejor clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import json\n",
    "\n",
    "def guardar_objeto_json(nombre_objeto, objeto):\n",
    "    ruta_archivo = os.path.join(nombre_objeto)\n",
    "    with open(ruta_archivo, 'w') as fp:\n",
    "        json.dump(objeto, fp)\n",
    "    \n",
    "def obtener_metricas(clasificador):\n",
    "    predicciones = clasificador.predict(X_test)\n",
    "    metricas = {\n",
    "        'accuracy': accuracy_score(Y_test, predicciones),\n",
    "        'precision': precision_score(Y_test, predicciones, pos_label='pos'),\n",
    "        'recall': recall_score(Y_test, predicciones, pos_label='pos'),\n",
    "        'f1': f1_score(Y_test, predicciones, pos_label='pos')\n",
    "    }\n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística con los parámetros:\n",
    "\n",
    "- random_state=42\n",
    "- solver='liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clasificador = LogisticRegression(random_state=42, solver='liblinear')\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'liblinear'\n",
    "metricas['random_state'] = 42\n",
    "guardar_objeto_json('metricas-reg-log3.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística con los parámetros:\n",
    "\n",
    "- solver='lbfgs'\n",
    "- n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = LogisticRegression(solver='lbfgs', n_jobs=-1)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'lbfgs'\n",
    "guardar_objeto_json('metricas-reg-log4.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística con los parámetros:\n",
    "\n",
    "- solver='newton-cg'\n",
    "- n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = LogisticRegression(solver='newton-cg', n_jobs=-1)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'newton-cg'\n",
    "guardar_objeto_json('metricas-reg-log5.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees con los parámetros:\n",
    "\n",
    "- n_estimators = 100\n",
    "- max_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK,Trials\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "clasificador = XGBClassifier(n_estimators=100, max_depth=10 )\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'GradientBoostedTrees'\n",
    "metricas['n_estimators'] = 100\n",
    "metricas['max_depth'] = 10\n",
    "guardar_objeto_json('metricas-GradientBoostedTrees.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Primero, se utilizará la optimización de hiper parámetros para encontrar la mejor combinación de valores. Esto no se hizo para los anteriores modelos debido a que su tiempo de entrenamiento es mucho mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "param_space = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(1,150)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(100,500)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n",
    "}\n",
    "\n",
    "def evaluator(params):\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    accurancy = cross_val_score(clf, X_train, Y_train).mean()\n",
    "    return {'loss': 1 - accurancy, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(evaluator, param_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest con los parámetros (obtenidos en la anterior celda):\n",
    "\n",
    "- criterion = gini\n",
    "- max_depth = 19\n",
    "- max_features = 147\n",
    "- n_estimators = 473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = RandomForestClassifier(criterion='gini', max_depth=19, max_features=147, n_estimators=473)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'RandomForest'\n",
    "metricas['n_estimators'] = 300\n",
    "metricas['max_depth'] = 15\n",
    "metricas['criterion'] = 'entropy'\n",
    "guardar_objeto_json('metricas-RandomForest.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar el mejor de los clasificadores como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ruta_archivo_clasificador = os.path.join('clasificador-regresion-logistica.pkl')\n",
    "archivo_clasificador = open(ruta_archivo_clasificador, 'wb')\n",
    "pickle.dump(clasificador, archivo_clasificador)\n",
    "archivo_clasificador.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el vocabulario del problema como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_vocabulario = os.path.join('vocabulario-problema.pkl')\n",
    "archivo_vacabulario = open(ruta_archivo_vocabulario, 'wb')\n",
    "pickle.dump(vocabulario_problema, archivo_vacabulario)\n",
    "archivo_vacabulario.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar las palabras de parada como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_palabras_parada = os.path.join('palabras-parada.pkl')\n",
    "archivo_palabras_parada = open(ruta_archivo_palabras_parada, 'wb')\n",
    "pickle.dump(palabras_de_parada_ingles, archivo_palabras_parada)\n",
    "archivo_palabras_parada.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
