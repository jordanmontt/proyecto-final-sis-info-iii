{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28046</th>\n",
       "      <td>Rented a batch of films from Blockbuster last ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38416</th>\n",
       "      <td>Dick Clement and Ian La Frenais have a solid h...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36230</th>\n",
       "      <td>Ugly shot, poorly scripted and amateurishly pa...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29802</th>\n",
       "      <td>Margret Laurence probably didn't intend on hav...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19238</th>\n",
       "      <td>***SPOILERS*** Well made and interesting film ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40691</th>\n",
       "      <td>I just saw this cartoon for the first time and...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22733</th>\n",
       "      <td>Last time I checked in here I think there was ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>carrot top in a full length movie, enough said...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34524</th>\n",
       "      <td>I'll give credit where credit is due, and say ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>Okay, so it was never going to change the worl...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Label\n",
       "28046  Rented a batch of films from Blockbuster last ...   neg\n",
       "38416  Dick Clement and Ian La Frenais have a solid h...   pos\n",
       "36230  Ugly shot, poorly scripted and amateurishly pa...   neg\n",
       "29802  Margret Laurence probably didn't intend on hav...   neg\n",
       "19238  ***SPOILERS*** Well made and interesting film ...   pos\n",
       "...                                                  ...   ...\n",
       "40691  I just saw this cartoon for the first time and...   pos\n",
       "22733  Last time I checked in here I think there was ...   pos\n",
       "8746   carrot top in a full length movie, enough said...   neg\n",
       "34524  I'll give credit where credit is due, and say ...   neg\n",
       "10499  Okay, so it was never going to change the worl...   neg\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "ruta_archivo = os.path.join(\"imdb_dataset.csv\")\n",
    "df_criticas = pd.read_csv(ruta_archivo, encoding='iso-8859-2').sample(1000, replace=False)\n",
    "#df_criticas = pd.read_csv(ruta_archivo, encoding='iso-8859-2')\n",
    "df_criticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras de parada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "palabras_de_parada_ingles = set(nltk.corpus.stopwords.words('english') + list(string.punctuation) + ['...', '..'])\n",
    "palabras_de_parada_ingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28046</th>\n",
       "      <td>rented batch films blockbuster last night firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38416</th>\n",
       "      <td>dick clement ian la frenais solid hit rate far...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36230</th>\n",
       "      <td>ugly shot poorly scripted amateurishly paced s...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29802</th>\n",
       "      <td>margret laurence probably intend novels adopte...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19238</th>\n",
       "      <td>spoilers well made interesting film alienated ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40691</th>\n",
       "      <td>saw cartoon first time recognized caricatures ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22733</th>\n",
       "      <td>last time checked think one comment glad peopl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>carrot top full length movie enough said reaso...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34524</th>\n",
       "      <td>give credit credit due say linda fiorentino gi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>okay never going change world bombed box offic...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Label\n",
       "28046  rented batch films blockbuster last night firs...   neg\n",
       "38416  dick clement ian la frenais solid hit rate far...   pos\n",
       "36230  ugly shot poorly scripted amateurishly paced s...   neg\n",
       "29802  margret laurence probably intend novels adopte...   neg\n",
       "19238  spoilers well made interesting film alienated ...   pos\n",
       "...                                                  ...   ...\n",
       "40691  saw cartoon first time recognized caricatures ...   pos\n",
       "22733  last time checked think one comment glad peopl...   pos\n",
       "8746   carrot top full length movie enough said reaso...   neg\n",
       "34524  give credit credit due say linda fiorentino gi...   neg\n",
       "10499  okay never going change world bombed box offic...   neg\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LimpiadorTexto es una clase que nosotros creamos.\n",
    "# Utilizamos la funcion normalizar aquÃ­, en el notebook. y tambiÃ©n en la API.\n",
    "# Por tanto, para no tener cÃ³digo repetido, abstraÃ­mos la lÃ³gica a una clase externa.\n",
    "from LimpiadorTexto import LimpiadorTexto\n",
    "limpiador = LimpiadorTexto(palabras_de_parada_ingles)\n",
    "df_criticas = df_criticas.apply(limpiador.normalizar_fila, axis=1)\n",
    "df_criticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener el vocabulario del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_vocabulario_problema(df_criticas_normalizadas):\n",
    "    cuerpo_texto_normalizado = ' '.join(df_criticas_normalizadas['Review'].tolist())\n",
    "    vocabulario_problema = cuerpo_texto_normalizado.split()\n",
    "    vocabulario_ordenado = sorted(set(vocabulario_problema))\n",
    "    return vocabulario_ordenado\n",
    "\n",
    "def obtener_vocabulario_problema_y_posicion(vocabulario_del_problema_ordenado):\n",
    "    vocabulario_y_posicion = {}\n",
    "    for i, token in enumerate(vocabulario_del_problema_ordenado):\n",
    "        vocabulario_y_posicion[token] = i\n",
    "    return vocabulario_y_posicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21079"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario_problema = obtener_vocabulario_problema_y_posicion(obtener_vocabulario_problema(df_criticas))\n",
    "len(vocabulario_problema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def obtener_one_hot_vector(critica, vocabulario_problema_y_posicion):\n",
    "    one_hot_vector = np.zeros(len(vocabulario_problema_y_posicion), dtype=int)\n",
    "    for token in critica.split():\n",
    "        one_hot_vector[vocabulario_problema_y_posicion[token]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots = []\n",
    "indices = []\n",
    "for indice, fila in df_criticas.iterrows():\n",
    "    one_hot = obtener_one_hot_vector(fila['Review'], vocabulario_problema)\n",
    "    indices.append(indice)\n",
    "    one_hots.append(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crear y entrenar el modelo predictivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# se cambia el nombre de la variable 'one_hots' para mas claridad\n",
    "X = one_hots\n",
    "Y = df_criticas['Label'].ravel()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train =  np.array(X_train)\n",
    "X_test =  np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberar la memoria RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "lst = [df_criticas]\n",
    "del df_criticas, one_hots, X, Y\n",
    "del lst\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar diferentes modelos con diferentes parÃ¡metros para encontrar el que produzca mejores resultados\n",
    "\n",
    "Se entrenarÃ¡n diferentes modelos con diferentes parÃ¡metros. Se guardarÃ¡ las mÃ©tricas de cada modelo en un archivo para que luego puedan ser comparadas, y asÃ­ determinar el mejor clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import json\n",
    "\n",
    "def guardar_objeto_json(nombre_objeto, objeto):\n",
    "    ruta_archivo = os.path.join(nombre_objeto)\n",
    "    with open(ruta_archivo, 'w') as fp:\n",
    "        json.dump(objeto, fp)\n",
    "    \n",
    "def obtener_metricas(clasificador):\n",
    "    predicciones = clasificador.predict(X_test)\n",
    "    metricas = {\n",
    "        'accuracy': accuracy_score(Y_test, predicciones),\n",
    "        'precision': precision_score(Y_test, predicciones, pos_label='pos'),\n",
    "        'recall': recall_score(Y_test, predicciones, pos_label='pos'),\n",
    "        'f1': f1_score(Y_test, predicciones, pos_label='pos')\n",
    "    }\n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegresiÃ³n logÃ­stica con los parÃ¡metros:\n",
    "\n",
    "- random_state=42\n",
    "- solver='liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clasificador = LogisticRegression(random_state=42, solver='liblinear')\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'liblinear'\n",
    "metricas['random_state'] = 42\n",
    "guardar_objeto_json('metricas-reg-log3.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegresiÃ³n logÃ­stica con los parÃ¡metros:\n",
    "\n",
    "- solver='lbfgs'\n",
    "- n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = LogisticRegression(solver='lbfgs', n_jobs=-1)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'lbfgs'\n",
    "guardar_objeto_json('metricas-reg-log4.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegresiÃ³n logÃ­stica con los parÃ¡metros:\n",
    "\n",
    "- solver='newton-cg'\n",
    "- n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = LogisticRegression(solver='newton-cg', n_jobs=-1)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'newton-cg'\n",
    "guardar_objeto_json('metricas-reg-log5.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees con los parÃ¡metros:\n",
    "\n",
    "- n_estimators = 100\n",
    "- max_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK,Trials\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "clasificador = XGBClassifier(n_estimators=100, max_depth=10 )\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'GradientBoostedTrees'\n",
    "metricas['n_estimators'] = 100\n",
    "metricas['max_depth'] = 10\n",
    "guardar_objeto_json('metricas-GradientBoostedTrees.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Primero, se utilizarÃ¡ la optimizaciÃ³n de hiper parÃ¡metros para encontrar la mejor combinaciÃ³n de valores. Esto no se hizo para los anteriores modelos debido a que su tiempo de entrenamiento es mucho mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "param_space = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(1,150)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(100,500)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n",
    "}\n",
    "\n",
    "def evaluator(params):\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    accurancy = cross_val_score(clf, X_train, Y_train).mean()\n",
    "    return {'loss': 1 - accurancy, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(evaluator, param_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest con los parÃ¡metros (obtenidos en la anterior celda):\n",
    "\n",
    "- criterion = gini\n",
    "- max_depth = 19\n",
    "- max_features = 147\n",
    "- n_estimators = 473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = RandomForestClassifier(criterion='gini', max_depth=19, max_features=147, n_estimators=473)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'RandomForest'\n",
    "metricas['n_estimators'] = 300\n",
    "metricas['max_depth'] = 15\n",
    "metricas['criterion'] = 'entropy'\n",
    "guardar_objeto_json('metricas-RandomForest.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar el mejor de los clasificadores como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ruta_archivo_clasificador = os.path.join('clasificador-regresion-logistica.pkl')\n",
    "archivo_clasificador = open(ruta_archivo_clasificador, 'wb')\n",
    "pickle.dump(clasificador, archivo_clasificador)\n",
    "archivo_clasificador.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el vocabulario del problema como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_vocabulario = os.path.join('vocabulario-problema.pkl')\n",
    "archivo_vacabulario = open(ruta_archivo_vocabulario, 'wb')\n",
    "pickle.dump(vocabulario_problema, archivo_vacabulario)\n",
    "archivo_vacabulario.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar las palabras de parada como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_palabras_parada = os.path.join('palabras-parada.pkl')\n",
    "archivo_palabras_parada = open(ruta_archivo_palabras_parada, 'wb')\n",
    "pickle.dump(palabras_de_parada_ingles, archivo_palabras_parada)\n",
    "archivo_palabras_parada.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
